{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.chdir('./5-1/')\n",
    "import rnn_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RNN forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_forward(xt, a_pre, params):\n",
    "\n",
    "    '''\n",
    "    input:\n",
    "        xt - x<t>, (n_x, m)\n",
    "        a_pre - a<t-1>, (n_a, m)\n",
    "        params - dict{\n",
    "            Wax - (n_a, n_x), xt --> at,\n",
    "            Waa - (n_a, n_a), a_pre --> at,\n",
    "            ba - (n_a, 1), xt,a_pre --> at,\n",
    "            Wya - (n_y, n_a), at --> yt_hat,\n",
    "            by - (n_y, 1), at --> yt_hat\n",
    "        }\n",
    "    output:\n",
    "        at - (n_a, m)\n",
    "        yt_hat - (n_y, m)\n",
    "        cache - cache for backpropagation, (at, a_pre, xt, params)\n",
    "    '''\n",
    "\n",
    "    Wax = params['Wax']\n",
    "    Waa = params['Waa']\n",
    "    Wya = params['Wya']\n",
    "    ba = params['ba']\n",
    "    by = params['by']\n",
    "\n",
    "    at = np.tanh(np.dot(Waa, a_pre) + np.dot(Wax, xt) + ba)\n",
    "    yt_hat = rnn_utils.softmax(np.dot(Wya, at) + by)\n",
    "    cache = (at, a_pre, xt, params)\n",
    "\n",
    "    return at, yt_hat, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(x, a0, params):\n",
    "\n",
    "    '''\n",
    "    input:\n",
    "        x - x, (n_x, m, T) : (features, samples, times)\n",
    "        a0 - zeros, (n_a, m)\n",
    "        params - dict{\n",
    "            Wax - (n_a, n_x), xt --> at,\n",
    "            Waa - (n_a, n_a), a_pre --> at,\n",
    "            ba - (n_a, 1), xt,a_pre --> at,\n",
    "            Wya - (n_y, n_a), at --> yt_hat,\n",
    "            by - (n_y, 1), at --> yt_hat\n",
    "        }\n",
    "    output:\n",
    "        a - (n_a, m, T), record at every time\n",
    "        y_hat - (n_y, m, T), record yt_hat every time\n",
    "        caches - cache for backpropagation, ([(at, a_pre, xt, params),...], x)\n",
    "    '''\n",
    "\n",
    "    caches = []\n",
    "    n_x, m, T = x.shape\n",
    "    n_y, n_a = params['Wya'].shape\n",
    "    a = np.zeros((n_a, m, T))\n",
    "    y_hat = np.zeros((n_y, m, T))\n",
    "    at = a0\n",
    "\n",
    "    for t in range(T):\n",
    "        at, yt_hat, cache = rnn_cell_forward(x[:,:,t], at, params)\n",
    "        a[:,:,t] = at\n",
    "        y_hat[:,:,t] = yt_hat\n",
    "        caches.append(cache)\n",
    "\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y_hat, caches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LSTM forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_pre, c_pre, params):\n",
    "\n",
    "    '''\n",
    "    input:\n",
    "        xt - (n_x, m)\n",
    "        a_pre - (n_a, m)\n",
    "        c_pre - (n_a, m)\n",
    "        params - dict{\n",
    "            Wf - (n_a, n_a + n_x), a_pre,xt --> gf,\n",
    "            bf - (n_a, 1), a_pre,xt --> gf,\n",
    "            Wu - (n_a, n_a + n_x), a_pre,xt --> gu,\n",
    "            bu - (n_a, 1), a_pre,xt --> gu,\n",
    "            Wc - (n_a, n_a + n_x), a_pre,xt --> ct_hat,\n",
    "            bc - (n_a, 1), a_pre,xt --> ct_hat,\n",
    "            Wo - (n_a, n_a + n_x), a_pre,xt --> go,\n",
    "            bo - (n_a, 1), a_pre,xt --> go,\n",
    "            Wy - (n_y, n_a), at --> yt_hat,\n",
    "            by - (n_y, 1), at --> yt_hat\n",
    "        }\n",
    "    output:\n",
    "        ct - (n_a, m)\n",
    "        at - (n_a, m)\n",
    "        yt_hat - (n_y, m)\n",
    "        cache - (a_pre, c_pre, xt, at, ct, params)\n",
    "    '''\n",
    "\n",
    "    Wf = params['Wf']\n",
    "    bf = params['bf']\n",
    "    Wu = params['Wu']\n",
    "    bu = params['bu']\n",
    "    Wc = params['Wc']\n",
    "    bc = params['bc']\n",
    "    Wo = params['Wo']\n",
    "    bo = params['bo']\n",
    "    Wy = params['Wy']\n",
    "    by = params['by']\n",
    "\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "\n",
    "    a_pre_xt = np.concatenate((a_pre, xt), axis=0)\n",
    "\n",
    "    gf = rnn_utils.sigmoid(np.dot(Wf, a_pre_xt) + bf)\n",
    "    gu = rnn_utils.sigmoid(np.dot(Wu, a_pre_xt) + bu)\n",
    "    ct_hat = np.tanh(np.dot(Wc, a_pre_xt) + bc)\n",
    "    go = rnn_utils.sigmoid(np.dot(Wo, a_pre_xt) + bo)\n",
    "    ct = c_pre * gf + ct_hat * gu\n",
    "    at = np.tanh(ct) *go\n",
    "    yt_hat = rnn_utils.softmax(np.dot(Wy, at) + by)\n",
    "\n",
    "    cache = (a_pre, c_pre, xt, at, ct, gf, gu, ct_hat, go, params)\n",
    "\n",
    "    return ct, at, yt_hat, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(x, a0, params):\n",
    "\n",
    "    '''\n",
    "    input:\n",
    "        x - (n_x, m, T)\n",
    "        a0 - (n_a, m)\n",
    "        params - dict{\n",
    "            Wf - (n_a, n_a + n_x), a_pre,xt --> gf,\n",
    "            bf - (n_a, 1), a_pre,xt --> gf,\n",
    "            Wu - (n_a, n_a + n_x), a_pre,xt --> gu,\n",
    "            bu - (n_a, 1), a_pre,xt --> gu,\n",
    "            Wc - (n_a, n_a + n_x), a_pre,xt --> ct_hat,\n",
    "            bc - (n_a, 1), a_pre,xt --> ct_hat,\n",
    "            Wo - (n_a, n_a + n_x), a_pre,xt --> go,\n",
    "            bo - (n_a, 1), a_pre,xt --> go,\n",
    "            Wy - (n_y, n_a), at --> yt_hat,\n",
    "            by - (n_y, 1), at --> yt_hat\n",
    "        }\n",
    "    output:\n",
    "        c - (n_a, m, T)\n",
    "        a - (n_a, m, T)\n",
    "        y_hat - (n_y, m, T)\n",
    "        caches - ([cache], x)\n",
    "    '''\n",
    "\n",
    "    caches = []\n",
    "    n_x, m, T = x.shape\n",
    "    n_y, n_a = params['Wy'].shape\n",
    "    a = np.zeros((n_a, m, T))\n",
    "    c = np.zeros((n_a, m, T))\n",
    "    y_hat = np.zeros((n_y, m, T))\n",
    "    at = a0\n",
    "    ct = np.zeros((n_a, m))\n",
    "\n",
    "    for t in range(T):\n",
    "        ct, at, yt_hat, cache = lstm_cell_forward(x[:,:,t], at, ct, params)\n",
    "        c[:,:,t] = ct\n",
    "        a[:,:,t] = at\n",
    "        y_hat[:,:,t] = yt_hat\n",
    "        caches.append(cache)\n",
    "\n",
    "    caches = (caches, x)\n",
    "\n",
    "    return c, a, y, caches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RNN backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_backward(dat, cache):\n",
    "\n",
    "    '''\n",
    "    input:\n",
    "        dat - (n_a, m), dJ/dat\n",
    "        cache - (at, a_pre, xt, params)\n",
    "    output:\n",
    "        grads_t - dict{\n",
    "            dxt - (n_x, m),\n",
    "            da_pre - (n_a, m),\n",
    "            dWaa - (n_a, n_a),\n",
    "            dWax - (n_a, n_x),\n",
    "            dba - (n_a, 1)\n",
    "        }\n",
    "    '''\n",
    "\n",
    "    at, a_pre, xt, params = cache\n",
    "    Wax = params['Wax']\n",
    "    Waa = params['Waa']\n",
    "    Wya = params['Wya']\n",
    "    ba = params['ba']\n",
    "    by = params['by']\n",
    "    \n",
    "    dWax = np.dot(dat * (1 - at**2), xt.T)\n",
    "    dWaa = np.dot(dat * (1 - at**2), a_pre.T)\n",
    "    dba = np.sum(dat * (1 - at**2), axis=1, keepdims=True)\n",
    "    dxt = np.dot(Wax.T, dat * (1 - at**2))\n",
    "    da_pre = np.dot(Waa.T, dat * (1 - at**2))\n",
    "\n",
    "    grads_t = {'dWax':dWax, 'dWaa':dWaa, 'dba':dba, 'dxt':dxt, 'da_pre':da_pre}\n",
    "\n",
    "    return grads_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(da, caches):\n",
    "\n",
    "    '''\n",
    "    input:\n",
    "        da - (n_a, m, T), dJ/dy_hat * dy_hat/da, without dat/da_pre\n",
    "        caches - cache for backpropagation, ([(at, a_pre, xt, params),...], x)\n",
    "    output:\n",
    "        grads - dict{\n",
    "            dx - (n_x, m, T),\n",
    "            da0 - (n_a, m),\n",
    "            dWax - (n_a, n_x),\n",
    "            dWaa - (n_a, n_a),\n",
    "            dba - (n_a, 1)\n",
    "        }\n",
    "    '''\n",
    "\n",
    "    caches, x = caches\n",
    "    n_x, m, T = x.shape\n",
    "    n_a, m, T = da.shape\n",
    "    \n",
    "    dx = np.zeros((n_x, m, T))\n",
    "    dWax = np.zeros((n_a, n_x))\n",
    "    dWaa = np.zeros((n_a, n_a))\n",
    "    dba = np.zeros((n_a, m))\n",
    "    da_pre = np.zeros((n_a, m))\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        grads_t = rnn_cell_backward(da[:,:,t] + da_pre, caches[t])\n",
    "        da_pre = grads_t['da_pre']\n",
    "        dx[:,:,t] = grads_t['dxt']\n",
    "        dWax += grads_t['dWax']\n",
    "        dWaa += grads_t['dWaa']\n",
    "        dba += grads_t['dba']\n",
    "\n",
    "    da0 = da_pre\n",
    "    \n",
    "    grads = {'dx':dx, 'da0':da0, 'dWax':dWax, 'dWaa':dWaa, 'dba':dba}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LSTM backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_backward(dat, dct, cache):\n",
    "\n",
    "    '''\n",
    "    input:\n",
    "        dat - (n_a, m)\n",
    "        dct - (n_a, m), ct --> c_pre, without at --> ct\n",
    "        cache - (a_pre, c_pre, xt, at, ct, gf, gu, ct_hat, go, params)\n",
    "    output:\n",
    "        grads_t - dict{\n",
    "            dxt - (n_x, m),\n",
    "            dc_pre - (n_a, m),\n",
    "            da_pre - (n_a, m),\n",
    "            dWf - (n_a, n_a+n_x),\n",
    "            dbf - (n_a, 1),\n",
    "            dWu - (n_a, n_a+n_x),\n",
    "            dbu - (n_a, 1),\n",
    "            dWc - (n_a, n_a+n_x),\n",
    "            dbc - (n_a, 1),\n",
    "            dWo - (n_a, n_a+n_x),\n",
    "            dbo - (n_a, 1)\n",
    "        }\n",
    "    '''\n",
    "\n",
    "    a_pre, c_pre, xt, at, ct, gf, gu, ct_hat, go, params = cache\n",
    "    n_x, m = xt.shape\n",
    "    n_a, m = at.shape\n",
    "\n",
    "    dgo_linear = dat * np.tanh(ct) * go * (1-go)    # gate output without sigmoid\n",
    "    dct_hat_linear = (dct * gu + go * (1 - np.tanh(ct)**2) * gu * dat) * (1 - ct_hat**2)    # without tanh\n",
    "    dgu_linear = (dct * ct_hat + dat * go * (1 - np.tanh(ct)**2)* ct_hat) * gu * (1-gu)\n",
    "    dgf_linear = (dct * c_pre + dat * go * (1 - np.tanh(ct)**2) * c_pre) * gf * (1-gf)\n",
    "    \n",
    "    a_pre_xt = np.concatenate((a_pre, xt), axis=0)\n",
    "    dWf = np.dot(dgu_linear, a_pre_xt.T)\n",
    "    dbf = np.sum(dgf_linear, axis=1, keepdims=True)\n",
    "    dWu = np.dot(dgu_linear, a_pre_xt.T)\n",
    "    dbu = np.sum(dgu_linear, axis=1, keepdims=True)\n",
    "    dWc = np.dot(dct_hat_linear, a_pre_xt.T)\n",
    "    dbc = np.sum(dct_hat_linear, axis=1, keepdims=True)\n",
    "    dWo = np.dot(dgo_linear, a_pre_xt.T)\n",
    "    dbo = np.sum(dgo_linear, axis=1, keepdims=True)\n",
    "\n",
    "    dc_pre = dct * gf + dat * go * (1 - np.tanh(ct)**2) * gf\n",
    "    da_pre = np.dot(params['Wf'][:, :n_a].T, dgf_linear) + \\\n",
    "             np.dot(params['Wu'][:, :n_a].T, dgu_linear) + \\\n",
    "             np.dot(params['Wc'][:, :n_a].T, dct_hat_linear) + \\\n",
    "             np.dot(params['Wo'][:, :n_a].T, dgo_linear)\n",
    "    dxt = np.dot(params['Wf'][:, n_a:].T, dgf_linear) + \\\n",
    "          np.dot(params['Wu'][:, n_a:].T, dgu_linear) + \\\n",
    "          np.dot(params['Wc'][:, n_a:].T, dct_hat_linear) + \\\n",
    "          np.dot(params['Wo'][:, n_a:].T, dgo_linear)\n",
    "\n",
    "    grads_t = {\n",
    "        'dxt': dxt,\n",
    "        'dc_pre': dc_pre,\n",
    "        'da_pre': da_pre,\n",
    "        'dWf': dWf,\n",
    "        'dbf': dbf,\n",
    "        'dWu': dWu,\n",
    "        'dbu': dbu,\n",
    "        'dWc': dWc,\n",
    "        'dbc': dbc,\n",
    "        'dWo': dWo,\n",
    "        'dbo': dbo\n",
    "    }\n",
    "\n",
    "    return grads_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(da, caches):\n",
    "\n",
    "    '''\n",
    "    input:\n",
    "        da - (n_a, m, T), dy_hat --> da, without dat --> da_pre\n",
    "        caches - ([cache], x)\n",
    "    output:\n",
    "        grads - dict{\n",
    "            dx - (n_x, m, T),\n",
    "            da0 - (n_a, m),\n",
    "            dWf, dWu, dWc, dWo - (n_a, n_a + n_x),\n",
    "            dbf, dbu, dbc, dbo - (n_a, 1)\n",
    "        }\n",
    "    '''\n",
    "\n",
    "    caches, x = caches\n",
    "    n_x, m, T = x.shape\n",
    "    n_a, m, T = da.shape\n",
    "\n",
    "    dx = np.zeros((n_x, m, T))\n",
    "    da_pre = np.zeros((n_a, m)) # T+1: da_pre\n",
    "    dc_pre = np.zeros((n_a, m)) # T+1: dc_pre\n",
    "    dWf = np.zeros((n_a, n_a + n_x))\n",
    "    dWu = np.zeros_like(dWf)\n",
    "    dWc = np.zeros_like(dWf)\n",
    "    dWo = np.zeros_like(dWf)\n",
    "    dbf = np.zeros((n_a, 1))\n",
    "    dbu = np.zeros_like(dbf)\n",
    "    dbc = np.zeros_like(dbf)\n",
    "    dbo = np.zeros_like(dbf)\n",
    "    \n",
    "    for t in reversed(range(T)):\n",
    "        grads_t = lstm_cell_backward(da[:,:,t] + da_pre, dc_pre, caches[t])\n",
    "        da_pre = grads_t['da_pre']\n",
    "        dc_pre = grads_t['dc_pre']\n",
    "        dx[:,:,t] = grads_t['dxt']\n",
    "        dWf += grads_t['dWf']\n",
    "        dWu += grads_t['dWu']\n",
    "        dWc += grads_t['dWc']\n",
    "        dWo += grads_t['dWo']\n",
    "        dbf += grads_t['dbf']\n",
    "        dbu += grads_t['dbu']\n",
    "        dbc += grads_t['dbc']\n",
    "        dbo += grads_t['dbo']\n",
    "\n",
    "    da0 = da_pre\n",
    "\n",
    "    grads = {\n",
    "        'dx': dx,\n",
    "        'da0': da0,\n",
    "        'dWf': dWf,\n",
    "        'dWu': dWu,\n",
    "        'dWc': dWc,\n",
    "        'dWo': dWo,\n",
    "        'dbf': dbf,\n",
    "        'dbu': dbu,\n",
    "        'dbc': dbc,\n",
    "        'dbo': dbo\n",
    "    }\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[None, 1, 2, 3]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### character rnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cllm_utils\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "aachenosaurus\naardonyx\nabdallahsaurus\nabelisaurus\nabrictosaurus\nabrosaurus\nabydosaurus\nacanthopholis\n['i', 'z', 'o', 'g', 'b', 'j', 'k', 't', 'q', 'l', 'c', 's', 'y', 'h', 'e', 'w', 'a', 'r', 'n', 'p', '\\n', 'v', 'f', 'd', 'm', 'u', 'x']\ndata_size = 19909, vocab_size = 27\n"
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data = data.lower()\n",
    "chars = list(set(data))\n",
    "data_size = len(data)\n",
    "vocab_size = len(chars)\n",
    "print(data[:100])\n",
    "print(chars)\n",
    "print(f'data_size = {data_size}, vocab_size = {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dict: one for mapping char to index, another for mapping index to char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
    }
   ],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(sorted(chars))}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(sorted(chars))}\n",
    "print(char_to_idx)\n",
    "print(idx_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### gradient clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(grads, maxvalue):\n",
    "\n",
    "    '''\n",
    "    input:\n",
    "        grads - dict of gradients\n",
    "        maxvalue - threshold of gradients, (-maxcalue, maxvalue)\n",
    "    output:\n",
    "        grads - grads after clip\n",
    "    '''\n",
    "\n",
    "    for k in grads.keys():\n",
    "        grads[k] = np.clip(grads[k], -maxvalue, maxvalue)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(params, char_to_idx, seed):\n",
    "\n",
    "    Waa = params['Waa']\n",
    "    Wax = params['Wax']\n",
    "    ba = params['b']\n",
    "    Wya = params['Wya']\n",
    "    by = params['by']\n",
    "\n",
    "    vocab_size = by.shape[0]    # = n_x = n_y\n",
    "    n_a = Waa.shape[0]\n",
    "    \n",
    "    xt = np.zeros((vocab_size, 1))  # initial x1\n",
    "    a_pre = np.zeros((n_a, 1))  # initial a0\n",
    "    indices = []    # list of index\n",
    "    idx = -1    # initial previous index to compare with flag\n",
    "    count = 0\n",
    "    flag = char_to_idx['\\n']    # index of '\\n'\n",
    "\n",
    "    while idx != flag and count < 50:\n",
    "        at = np.tanh(np.dot(Waa, a_pre) + np.dot(Wax, xt) + ba)\n",
    "        z = np.dot(Wya, at) + by\n",
    "        yt_hat = cllm_utils.softmax(z)  # (vocab_size, 1)\n",
    "\n",
    "        np.random.seed(seed + count)\n",
    "        idx = np.random.choice(np.arange(vocab_size), p=yt_hat.flatten())\n",
    "        indices.append(idx)\n",
    "        # update xt, a_pre\n",
    "        xt = np.zeros((vocab_size, 1))\n",
    "        xt[idx, 0] = 1\n",
    "        a_pre = at\n",
    "        count += 1\n",
    "\n",
    "    if count == 50 and idx != flag:\n",
    "        indices.append(flag)\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SGD\n",
    "#### notice: in cllm_utils.py, a, x, y_hat --> dict; X, Y --> index list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a0, params, alpha=0.01):\n",
    "\n",
    "    '''\n",
    "    input:\n",
    "        X - one sample of data, int list, [None, 1,2,3,...]\n",
    "        Y - one sample of data as X, int list, [1,2,3,...,0]\n",
    "        a0 - initial a_pre\n",
    "        params - dict of Waa,Wax,Wya,b,by\n",
    "        alpha - learning rate\n",
    "    output:\n",
    "        loss\n",
    "        grads - dict of gradients\n",
    "        params - dict of params updated\n",
    "        at - last at\n",
    "    '''\n",
    "\n",
    "    # forward propagation\n",
    "    loss, cache = cllm_utils.rnn_forward(X, Y, a0=a0, parameters=params)\n",
    "\n",
    "    # backward propagation through time\n",
    "    grads, a = cllm_utils.rnn_backward(X, Y, parameters=params, cache=cache)\n",
    "\n",
    "    # gradients clips\n",
    "    grads = clip(grads, 5)\n",
    "\n",
    "    # update params\n",
    "    params = cllm_utils.update_parameters(params, grads, alpha)\n",
    "\n",
    "    return loss, grads, params, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['aachenosaurus', 'aardonyx', 'abdallahsaurus', 'abelisaurus', 'abrictosaurus', 'abrosaurus', 'abydosaurus', 'acanthopholis', 'achelousaurus', 'acheroraptor']\n1536\n"
    }
   ],
   "source": [
    "examples = data.split('\\n')\n",
    "print(examples[:10])\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, loops=4000, n_a=50, sample_times=7):\n",
    "\n",
    "    '''\n",
    "    input:\n",
    "        data - dataset\n",
    "        loops - iterate times\n",
    "        n_a - n_a\n",
    "        sample_times - times of sample to validate model\n",
    "    output:\n",
    "        params\n",
    "    '''\n",
    "\n",
    "    vocab_size = len(set(data))\n",
    "    chars = list(set(data))\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(sorted(chars))}\n",
    "    idx_to_char = {idx: char for idx, char in enumerate(sorted(chars))}\n",
    "\n",
    "    params = cllm_utils.initialize_parameters(n_a, vocab_size, vocab_size)\n",
    "    a0 = np.zeros((n_a, 1))\n",
    "\n",
    "    examples = data.split('\\n')\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "\n",
    "    for i in range(loops):\n",
    "        index = i % len(examples)\n",
    "        X = [None] + [char_to_idx[char] for char in examples[index]]\n",
    "        Y = [char_to_idx[char] for char in examples[index]] + [char_to_idx['\\n']]\n",
    "\n",
    "        loss, grads, params, at = optimize(X, Y, a0, params)\n",
    "        \n",
    "        if i % 2000 == 0:\n",
    "            print(f'No.{i+1} iteration loss: {loss}')\n",
    "            seed = 0\n",
    "            for j in range(sample_times):\n",
    "                indices = sample(params, char_to_idx, seed)\n",
    "                cllm_utils.print_sample(indices, idx_to_char)\n",
    "                seed += 1\n",
    "    \n",
    "    return params, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "No.1 iteration loss: 39.548881516330404\nNkknzexbw\nKknzexbw\nKnzexbw\nNzexbw\nZexbw\nExbw\nXbw\nNo.2001 iteration loss: 22.938709387824456\nMhgnygtas\nIklyhtas\nKlygtas\nMyftas\nYesaubus\nDuctarahopsaurus\nTarasanrus\ntotal: run 9.10140061378479 seconds.\nFinally, loss: 13.802386450251815\n"
    }
   ],
   "source": [
    "start = time.time()\n",
    "params, loss = model(data)\n",
    "end = time.time()\n",
    "print(f'total: run {end-start} seconds.')\n",
    "print(f'Finally, loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}